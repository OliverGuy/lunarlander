{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "superb-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dima/.pyenv/versions/3.8.6/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "\n",
    "from DQN_parametrized import DQN_parametrized \n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from memory import ReplayMemory\n",
    "from DQN import DQN\n",
    "from preprocessing import get_screen\n",
    "from utils import select_action, plot_scores\n",
    "from training import optimize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complete-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "init_screen = get_screen(env)\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "solar-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_booster = None\n",
    "target_net = None\n",
    "\n",
    "def callback(study, trial):\n",
    "    global best_booster\n",
    "    if study.best_trial == trial:\n",
    "        best_booster = target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "painful-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    global target_net\n",
    "    \n",
    "    pred_net = DQN_parametrized(screen_height, screen_width, n_actions, trial).to(device)\n",
    "    \n",
    "    target_net = DQN_parametrized(screen_height, screen_width, n_actions, trial).to(device)\n",
    "    target_net.load_state_dict(pred_net.state_dict())\n",
    "    target_net.eval() \n",
    "    \n",
    "    REPLAY_MEMORY_SIZE = 1000\n",
    "    memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "    TARGET_UPDATE = 1000  # period of target network update\n",
    "    optimizer = optim.RMSprop(pred_net.parameters())\n",
    "\n",
    "    num_episodes = 40\n",
    "    episode_rewards = []\n",
    "    steps = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        last_screen = get_screen(env).to(device)\n",
    "        current_screen = get_screen(env).to(device)\n",
    "        state = current_screen - last_screen\n",
    "        episode_rewards.append(0)\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Select and perform an action\n",
    "            action = select_action(pred_net, state, n_actions).to(device)\n",
    "            _, reward, done, _ = env.step(action.item())  # our states are screenshot differences\n",
    "            episode_rewards[-1] += reward\n",
    "\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            current_screen = get_screen(env).to(device)\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model(device, pred_net, target_net, optimizer, memory)\n",
    "            steps += 1\n",
    "\n",
    "            if steps == TARGET_UPDATE:  # update the target net weights\n",
    "                steps = 0\n",
    "                target_net.load_state_dict(pred_net.state_dict())\n",
    "        print(i_episode, 'reward:', episode_rewards[-1])\n",
    "    return max(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "senior-opera",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-07 23:26:40,576]\u001b[0m A new study created in memory with name: no-name-94298053-3875-4dc8-affd-048a11b079f6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "thick-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reward: -193.47168327341757\n",
      "1 reward: -33.93075403572931\n",
      "2 reward: -125.53032236734325\n",
      "3 reward: -85.9964378127736\n",
      "4 reward: -85.21963042188071\n",
      "5 reward: -139.6636811559829\n",
      "6 reward: -143.53591462379052\n",
      "7 reward: -127.60085857257738\n",
      "8 reward: -151.39647231179842\n",
      "9 reward: -174.15802717839549\n",
      "10 reward: -130.463864975585\n",
      "11 reward: -138.8973573312558\n",
      "12 reward: -115.6110347071672\n",
      "13 reward: -93.9474833255259\n",
      "14 reward: -113.10385365209497\n",
      "15 reward: -82.63073380104468\n",
      "16 reward: -109.76363388470351\n",
      "17 reward: -191.9516804669805\n",
      "18 reward: -157.4038667464405\n",
      "19 reward: -196.9134134835227\n",
      "20 reward: -140.31488816142468\n",
      "21 reward: -117.85225956469046\n",
      "22 reward: -120.00872413968408\n",
      "23 reward: -230.35936216967588\n",
      "24 reward: -135.9250429780692\n",
      "25 reward: -139.11034691034814\n",
      "26 reward: -167.43011955825966\n",
      "27 reward: -137.21390554548702\n",
      "28 reward: -96.54490678551667\n",
      "29 reward: -172.96694417078925\n",
      "30 reward: -114.86960174349309\n",
      "31 reward: -148.03251359120003\n",
      "32 reward: -136.28972239804898\n",
      "33 reward: -190.6831902884686\n",
      "34 reward: -119.10555266948123\n",
      "35 reward: -216.09026567027502\n",
      "36 reward: -106.50248867317671\n",
      "37 reward: -82.94945132446541\n",
      "38 reward: 11.229728994714918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 00:37:11,464]\u001b[0m Trial 0 finished with value: 11.229728994714918 and parameters: {'n_layers': 2, 'kernel_size_0': 3, 'stride_0': 1, 'padding_0': 1, 'out_channel_0': 6, 'activation_0': 'hardswish', 'kernel_size_1': 2, 'stride_1': 1, 'padding_1': 0, 'out_channel_1': 7, 'activation_1': 'hardswish', 'n_layers_1': 1, 'out_features_1_0': 879, 'activation_1_0': 'selu', 'hidden_size': 788, 'n_layers_2': 0}. Best is trial 0 with value: 11.229728994714918.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -20.556073236276333\n",
      "0 reward: -207.60870996539677\n",
      "1 reward: -313.86895318533414\n",
      "2 reward: -600.8460695213594\n",
      "3 reward: -331.364940465624\n",
      "4 reward: -182.34476676025372\n",
      "5 reward: -156.9262924345816\n",
      "6 reward: -112.50993007984273\n",
      "7 reward: 20.944303567334757\n",
      "8 reward: -73.91441064604464\n",
      "9 reward: -337.7737754643307\n",
      "10 reward: -78.38580072723423\n",
      "11 reward: -217.04817884860358\n",
      "12 reward: -96.4265099988959\n",
      "13 reward: -55.277292480615316\n",
      "14 reward: -96.19210808746936\n",
      "15 reward: -403.4045545220032\n",
      "16 reward: -172.3938272483763\n",
      "17 reward: -96.62519671819787\n",
      "18 reward: -190.38922296799993\n",
      "19 reward: -86.38395201643011\n",
      "20 reward: -100.42168243960153\n",
      "21 reward: -163.06615101054797\n",
      "22 reward: -211.87885803560437\n",
      "23 reward: -385.43541081364657\n",
      "24 reward: -51.18181182389756\n",
      "25 reward: 89.55769618282758\n",
      "26 reward: -141.47708538248176\n",
      "27 reward: -338.9098641363929\n",
      "28 reward: -218.3744051570914\n",
      "29 reward: -402.7894415904489\n",
      "30 reward: -575.5183184910869\n",
      "31 reward: -46.01859005475627\n",
      "32 reward: -443.3608184547229\n",
      "33 reward: 6.809597542769268\n",
      "34 reward: -119.05450991418469\n",
      "35 reward: -73.97158277226143\n",
      "36 reward: -329.58701938419955\n",
      "37 reward: -264.520603648101\n",
      "38 reward: -66.97142619032479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 10:27:28,390]\u001b[0m Trial 1 finished with value: 89.55769618282758 and parameters: {'n_layers': 1, 'kernel_size_0': 2, 'stride_0': 1, 'padding_0': 1, 'out_channel_0': 10, 'activation_0': 'relu', 'n_layers_1': 1, 'out_features_1_0': 674, 'activation_1_0': 'hardswish', 'hidden_size': 408, 'n_layers_2': 1, 'out_features_2_0': 61, 'activation_2_0': 'selu'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -210.53643778946778\n",
      "0 reward: -392.5134232122845\n",
      "1 reward: -466.2805943303826\n",
      "2 reward: -111.66582676720854\n",
      "3 reward: -145.88329243619103\n",
      "4 reward: -143.65277519532282\n",
      "5 reward: -123.51308321300189\n",
      "6 reward: -108.50169274716374\n",
      "7 reward: -174.108587575106\n",
      "8 reward: -156.24981364245548\n",
      "9 reward: -181.30605949573874\n",
      "10 reward: -28.25240561809329\n",
      "11 reward: -128.80620503648225\n",
      "12 reward: -141.35245499065527\n",
      "13 reward: -127.15753615211126\n",
      "14 reward: -163.06724285631674\n",
      "15 reward: -140.12047008982935\n",
      "16 reward: -196.99131210315664\n",
      "17 reward: 12.78052063942971\n",
      "18 reward: -173.48740915192764\n",
      "19 reward: -158.63812567433888\n",
      "20 reward: -184.0713617485496\n",
      "21 reward: -149.71688855131316\n",
      "22 reward: -144.59591937947943\n",
      "23 reward: -185.8713869050146\n",
      "24 reward: -162.58136034840427\n",
      "25 reward: -155.67974463169924\n",
      "26 reward: -88.0432448146961\n",
      "27 reward: -136.93985467922226\n",
      "28 reward: -133.42409811934243\n",
      "29 reward: -157.40336314127842\n",
      "30 reward: -130.8345170525712\n",
      "31 reward: -147.7306119386687\n",
      "32 reward: -164.5419668651997\n",
      "33 reward: -119.68862653856587\n",
      "34 reward: -92.79035209013733\n",
      "35 reward: -107.30927291005032\n",
      "36 reward: -143.13038403294675\n",
      "37 reward: -116.64802081861254\n",
      "38 reward: -122.80932209001143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 10:34:24,315]\u001b[0m Trial 2 finished with value: 12.78052063942971 and parameters: {'n_layers': 1, 'kernel_size_0': 3, 'stride_0': 3, 'padding_0': 0, 'out_channel_0': 8, 'activation_0': 'hardswish', 'n_layers_1': 2, 'out_features_1_0': 303, 'activation_1_0': 'selu', 'out_features_1_1': 914, 'activation_1_1': 'selu', 'hidden_size': 517, 'n_layers_2': 1, 'out_features_2_0': 501, 'activation_2_0': 'relu'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -111.25121187070027\n",
      "0 reward: -293.3812676931407\n",
      "1 reward: -329.46455026710714\n",
      "2 reward: -123.57610698256867\n",
      "3 reward: -121.43740493300001\n",
      "4 reward: -13.61182391707446\n",
      "5 reward: -36.48197448042211\n",
      "6 reward: -148.07477265143137\n",
      "7 reward: -160.6386668870947\n",
      "8 reward: -131.85019524068167\n",
      "9 reward: -153.05605285422155\n",
      "10 reward: -148.85604866032216\n",
      "11 reward: -147.15281337730426\n",
      "12 reward: -169.0220883068876\n",
      "13 reward: -116.99364252131564\n",
      "14 reward: -149.27962230144485\n",
      "15 reward: -93.269793253842\n",
      "16 reward: -160.71500960167046\n",
      "17 reward: -120.3361408894828\n",
      "18 reward: -109.87127786857681\n",
      "19 reward: -120.83635580417686\n",
      "20 reward: -177.5805178021271\n",
      "21 reward: -122.16948298865901\n",
      "22 reward: -88.94245269819586\n",
      "23 reward: -156.80879153429572\n",
      "24 reward: -124.45599886855152\n",
      "25 reward: -124.1618079578765\n",
      "26 reward: -139.62274536829756\n",
      "27 reward: -223.94561502175597\n",
      "28 reward: 28.721462078307127\n",
      "29 reward: -122.17172686405713\n",
      "30 reward: -90.50154313340947\n",
      "31 reward: -84.32007508732895\n",
      "32 reward: -171.0277874388908\n",
      "33 reward: -129.43078146864354\n",
      "34 reward: -320.64457143345476\n",
      "35 reward: -103.39891887625205\n",
      "36 reward: -126.7502665765844\n",
      "37 reward: -115.42393167271275\n",
      "38 reward: -114.22155254632989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 10:44:25,823]\u001b[0m Trial 3 finished with value: 28.721462078307127 and parameters: {'n_layers': 3, 'kernel_size_0': 4, 'stride_0': 3, 'padding_0': 1, 'out_channel_0': 10, 'activation_0': 'relu', 'kernel_size_1': 4, 'stride_1': 1, 'padding_1': 0, 'out_channel_1': 5, 'activation_1': 'selu', 'kernel_size_2': 4, 'stride_2': 3, 'padding_2': 0, 'out_channel_2': 3, 'activation_2': 'hardswish', 'n_layers_1': 2, 'out_features_1_0': 528, 'activation_1_0': 'relu', 'out_features_1_1': 683, 'activation_1_1': 'selu', 'hidden_size': 572, 'n_layers_2': 1, 'out_features_2_0': 280, 'activation_2_0': 'hardswish'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -140.16799407385034\n",
      "0 reward: -123.9062995375327\n",
      "1 reward: -313.1052601948969\n",
      "2 reward: -322.86631989948836\n",
      "3 reward: -424.277624852986\n",
      "4 reward: -64.2278630154464\n",
      "5 reward: -427.24165710764146\n",
      "6 reward: -162.7531623099241\n",
      "7 reward: -111.35586472283389\n",
      "8 reward: -237.67947943697024\n",
      "9 reward: -143.7896966711169\n",
      "10 reward: -512.149708702523\n",
      "11 reward: -84.98095793793456\n",
      "12 reward: -288.2443214305831\n",
      "13 reward: -458.39111825913415\n",
      "14 reward: -158.17528048550497\n",
      "15 reward: -154.4813151081833\n",
      "16 reward: -333.8562958140792\n",
      "17 reward: -280.7186792812609\n",
      "18 reward: -94.80517996660447\n",
      "19 reward: -15.123342670678227\n",
      "20 reward: -248.35955875068356\n",
      "21 reward: -411.85054453570524\n",
      "22 reward: -287.7704435183197\n",
      "23 reward: -90.82678953802665\n",
      "24 reward: -254.72781796827772\n",
      "25 reward: -68.26285349595396\n",
      "26 reward: -83.7790241319629\n",
      "27 reward: -97.49472470745549\n",
      "28 reward: -103.03016442240985\n",
      "29 reward: -321.1561263104527\n",
      "30 reward: -168.20793197973882\n",
      "31 reward: -90.85873154886276\n",
      "32 reward: -80.9483985498322\n",
      "33 reward: -248.75040543665986\n",
      "34 reward: -117.21981721673846\n",
      "35 reward: -72.07495127902789\n",
      "36 reward: -573.4148539383168\n",
      "37 reward: -251.37134906611786\n",
      "38 reward: -321.24206792213613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 11:03:50,301]\u001b[0m Trial 4 finished with value: -15.123342670678227 and parameters: {'n_layers': 1, 'kernel_size_0': 3, 'stride_0': 2, 'padding_0': 1, 'out_channel_0': 5, 'activation_0': 'relu', 'n_layers_1': 3, 'out_features_1_0': 978, 'activation_1_0': 'selu', 'out_features_1_1': 403, 'activation_1_1': 'hardswish', 'out_features_1_2': 405, 'activation_1_2': 'selu', 'hidden_size': 321, 'n_layers_2': 0}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -229.70584908445386\n",
      "0 reward: -183.81488663125242\n",
      "1 reward: -270.3625824364141\n",
      "2 reward: -600.04259323113\n",
      "3 reward: -182.5758248763142\n",
      "4 reward: -244.23304305724193\n",
      "5 reward: -14.634199887584401\n",
      "6 reward: -122.84781004315025\n",
      "7 reward: -188.42182495540652\n",
      "8 reward: -270.35900341171316\n",
      "9 reward: -449.451702645551\n",
      "10 reward: -272.9419130570577\n",
      "11 reward: -378.3427840689516\n",
      "12 reward: -92.58680523515334\n",
      "13 reward: -280.46068555933573\n",
      "14 reward: -271.9853796642301\n",
      "15 reward: -547.0698810743961\n",
      "16 reward: -352.8948246046069\n",
      "17 reward: -120.10497363050513\n",
      "18 reward: -321.3961979791286\n",
      "19 reward: -286.9502338095491\n",
      "20 reward: -290.3321656829861\n",
      "21 reward: -310.989557915134\n",
      "22 reward: -291.55578812222353\n",
      "23 reward: -343.054702370697\n",
      "24 reward: -118.95735012076678\n",
      "25 reward: -461.0157388435436\n",
      "26 reward: -101.55958942162482\n",
      "27 reward: -97.95547288502152\n",
      "28 reward: -65.95828903598965\n",
      "29 reward: -67.42320982696533\n",
      "30 reward: -102.48954691960994\n",
      "31 reward: -271.69894998523273\n",
      "32 reward: -318.4047317847989\n",
      "33 reward: -439.4377433260971\n",
      "34 reward: -310.1276954272755\n",
      "35 reward: -101.2137736469269\n",
      "36 reward: -639.8034811610521\n",
      "37 reward: -399.76523924125786\n",
      "38 reward: -500.4299238857761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 11:15:33,653]\u001b[0m Trial 5 finished with value: -14.634199887584401 and parameters: {'n_layers': 3, 'kernel_size_0': 2, 'stride_0': 2, 'padding_0': 1, 'out_channel_0': 4, 'activation_0': 'relu', 'kernel_size_1': 2, 'stride_1': 3, 'padding_1': 0, 'out_channel_1': 8, 'activation_1': 'relu', 'kernel_size_2': 2, 'stride_2': 3, 'padding_2': 0, 'out_channel_2': 12, 'activation_2': 'hardswish', 'n_layers_1': 3, 'out_features_1_0': 780, 'activation_1_0': 'hardswish', 'out_features_1_1': 799, 'activation_1_1': 'selu', 'out_features_1_2': 532, 'activation_1_2': 'relu', 'hidden_size': 317, 'n_layers_2': 0}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -426.5520917535929\n",
      "0 reward: -104.44399432340964\n",
      "1 reward: -269.20212511968737\n",
      "2 reward: -559.0559703096801\n",
      "3 reward: -47.26525302351152\n",
      "4 reward: -279.7623087785264\n",
      "5 reward: -288.5211394450386\n",
      "6 reward: -321.2970505884781\n",
      "7 reward: -265.55536509411013\n",
      "8 reward: -203.7717901161034\n",
      "9 reward: -80.06810033160112\n",
      "10 reward: -204.50507424788606\n",
      "11 reward: -331.1950144365364\n",
      "12 reward: -187.48919439346193\n",
      "13 reward: -114.19808160634346\n",
      "14 reward: -239.36419684573679\n",
      "15 reward: -300.2955966702044\n",
      "16 reward: -99.28211521225516\n",
      "17 reward: -221.11488444457882\n",
      "18 reward: -210.04053898825381\n",
      "19 reward: -625.7009341342994\n",
      "20 reward: -365.5361747212397\n",
      "21 reward: -249.34505052133855\n",
      "22 reward: -403.0466867237794\n",
      "23 reward: -217.18827988034906\n",
      "24 reward: -369.9782061952778\n",
      "25 reward: -80.31032130451068\n",
      "26 reward: -112.9080398996971\n",
      "27 reward: -125.53704100061364\n",
      "28 reward: -372.59614158687754\n",
      "29 reward: -584.536605988282\n",
      "30 reward: -37.172513972460905\n",
      "31 reward: -461.23393881950614\n",
      "32 reward: -453.5197866554886\n",
      "33 reward: -321.2979180595578\n",
      "34 reward: -404.08972903870034\n",
      "35 reward: -54.0236795174544\n",
      "36 reward: -117.00390545336892\n",
      "37 reward: -213.25194739699626\n",
      "38 reward: -445.3743920988731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 11:29:34,484]\u001b[0m Trial 6 finished with value: -37.172513972460905 and parameters: {'n_layers': 3, 'kernel_size_0': 2, 'stride_0': 2, 'padding_0': 1, 'out_channel_0': 6, 'activation_0': 'hardswish', 'kernel_size_1': 4, 'stride_1': 2, 'padding_1': 1, 'out_channel_1': 8, 'activation_1': 'hardswish', 'kernel_size_2': 2, 'stride_2': 3, 'padding_2': 0, 'out_channel_2': 4, 'activation_2': 'selu', 'n_layers_1': 3, 'out_features_1_0': 384, 'activation_1_0': 'hardswish', 'out_features_1_1': 908, 'activation_1_1': 'hardswish', 'out_features_1_2': 812, 'activation_1_2': 'hardswish', 'hidden_size': 536, 'n_layers_2': 1, 'out_features_2_0': 175, 'activation_2_0': 'hardswish'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -494.92980497318854\n",
      "0 reward: -331.4510203168837\n",
      "1 reward: -219.6706553457633\n",
      "2 reward: -115.64161855326806\n",
      "3 reward: -187.88332890691632\n",
      "4 reward: -184.62183729512898\n",
      "5 reward: -174.40227361043327\n",
      "6 reward: -161.00688789370105\n",
      "7 reward: -139.04084499378365\n",
      "8 reward: -177.13041684346314\n",
      "9 reward: -150.28862991771166\n",
      "10 reward: -109.35005252651\n",
      "11 reward: -102.00510988529474\n",
      "12 reward: -111.8863556264128\n",
      "13 reward: -163.57565534156987\n",
      "14 reward: -173.3338393199498\n",
      "15 reward: -185.66505233485339\n",
      "16 reward: 3.2884780951672212\n",
      "17 reward: -130.3340110721311\n",
      "18 reward: -136.24928479218468\n",
      "19 reward: -187.44801202115363\n",
      "20 reward: -174.09409707649883\n",
      "21 reward: -139.84284754226763\n",
      "22 reward: -135.82352871251098\n",
      "23 reward: -109.1705676087405\n",
      "24 reward: -185.74468118169327\n",
      "25 reward: -142.8450319662485\n",
      "26 reward: -137.85486817178824\n",
      "27 reward: -152.79247766187163\n",
      "28 reward: -111.39149481131695\n",
      "29 reward: -87.25998206308782\n",
      "30 reward: -151.21500283951445\n",
      "31 reward: -186.10178482897308\n",
      "32 reward: -133.60838322205367\n",
      "33 reward: -137.6474149776849\n",
      "34 reward: -137.0819975532676\n",
      "35 reward: -44.27537454858286\n",
      "36 reward: -156.85171263152353\n",
      "37 reward: -119.23017688045398\n",
      "38 reward: -159.26896918208305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 11:41:32,241]\u001b[0m Trial 7 finished with value: 3.2884780951672212 and parameters: {'n_layers': 3, 'kernel_size_0': 4, 'stride_0': 2, 'padding_0': 0, 'out_channel_0': 11, 'activation_0': 'hardswish', 'kernel_size_1': 4, 'stride_1': 3, 'padding_1': 1, 'out_channel_1': 12, 'activation_1': 'selu', 'kernel_size_2': 3, 'stride_2': 3, 'padding_2': 0, 'out_channel_2': 9, 'activation_2': 'selu', 'n_layers_1': 1, 'out_features_1_0': 782, 'activation_1_0': 'selu', 'hidden_size': 683, 'n_layers_2': 1, 'out_features_2_0': 368, 'activation_2_0': 'hardswish'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -164.5566862071659\n",
      "0 reward: -527.8058292250537\n",
      "1 reward: -260.16985881888866\n",
      "2 reward: -124.68699234264483\n",
      "3 reward: -138.74741042378278\n",
      "4 reward: -139.69563908272755\n",
      "5 reward: -111.80002452241655\n",
      "6 reward: -211.35763108391023\n",
      "7 reward: -182.63377844598068\n",
      "8 reward: -113.07572972513749\n",
      "9 reward: -89.23190244369017\n",
      "10 reward: -114.88766998575792\n",
      "11 reward: -143.39696017863477\n",
      "12 reward: -119.59327972666391\n",
      "13 reward: -150.49656137555218\n",
      "14 reward: -117.79295648861708\n",
      "15 reward: -123.57519836076916\n",
      "16 reward: -199.00480110474135\n",
      "17 reward: -29.366305649374013\n",
      "18 reward: -94.81819994812095\n",
      "19 reward: -126.2974225643936\n",
      "20 reward: -115.95087044689933\n",
      "21 reward: -154.30538424555888\n",
      "22 reward: -144.600638414635\n",
      "23 reward: -130.93984502006617\n",
      "24 reward: -172.2453900666236\n",
      "25 reward: -142.68307582511875\n",
      "26 reward: -190.62764625505582\n",
      "27 reward: -142.26437101457117\n",
      "28 reward: -101.94915339990133\n",
      "29 reward: -114.70974020231365\n",
      "30 reward: -110.77021728945081\n",
      "31 reward: -77.94445805720687\n",
      "32 reward: -126.1922235316077\n",
      "33 reward: -129.88491362801358\n",
      "34 reward: -98.80715836438563\n",
      "35 reward: -125.09081800439284\n",
      "36 reward: -151.00459063390628\n",
      "37 reward: -168.94948450105957\n",
      "38 reward: -167.30564216501517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 12:06:13,270]\u001b[0m Trial 8 finished with value: -29.366305649374013 and parameters: {'n_layers': 1, 'kernel_size_0': 3, 'stride_0': 2, 'padding_0': 1, 'out_channel_0': 10, 'activation_0': 'selu', 'n_layers_1': 1, 'out_features_1_0': 998, 'activation_1_0': 'selu', 'hidden_size': 562, 'n_layers_2': 2, 'out_features_2_0': 68, 'activation_2_0': 'hardswish', 'out_features_2_1': 261, 'activation_2_1': 'selu'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -173.71696489504387\n",
      "0 reward: -102.56577583043267\n",
      "1 reward: -69.31084706393995\n",
      "2 reward: -333.60852052764074\n",
      "3 reward: -601.2196437217099\n",
      "4 reward: -465.15243153770217\n",
      "5 reward: -519.7299100056946\n",
      "6 reward: -208.57394093623378\n",
      "7 reward: -78.82472829384193\n",
      "8 reward: -224.62574900563118\n",
      "9 reward: -297.73325703647964\n",
      "10 reward: -334.01722873537597\n",
      "11 reward: -469.03831958546743\n",
      "12 reward: -146.43755409062712\n",
      "13 reward: -105.22910498159027\n",
      "14 reward: -5.985249337974366\n",
      "15 reward: -457.64545188925666\n",
      "16 reward: -282.90753736750514\n",
      "17 reward: -372.8150189340222\n",
      "18 reward: -457.0461085447802\n",
      "19 reward: -95.3883250433248\n",
      "20 reward: -67.94212103729771\n",
      "21 reward: -69.11022300324169\n",
      "22 reward: -454.54893011556845\n",
      "23 reward: -32.854170772009255\n",
      "24 reward: -463.77722987075634\n",
      "25 reward: -122.52978291219415\n",
      "26 reward: -96.35051950707523\n",
      "27 reward: -459.04492343209574\n",
      "28 reward: -79.81564194677128\n",
      "29 reward: -437.5645744385903\n",
      "30 reward: -68.23122869119197\n",
      "31 reward: -279.91029829138864\n",
      "32 reward: -671.4598861371085\n",
      "33 reward: -592.099318429141\n",
      "34 reward: -510.4707895808759\n",
      "35 reward: -421.2696471723559\n",
      "36 reward: -226.34441422486026\n",
      "37 reward: -392.03056522226444\n",
      "38 reward: -356.3924984389577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-08 19:25:46,918]\u001b[0m Trial 9 finished with value: -5.985249337974366 and parameters: {'n_layers': 2, 'kernel_size_0': 3, 'stride_0': 1, 'padding_0': 1, 'out_channel_0': 9, 'activation_0': 'selu', 'kernel_size_1': 4, 'stride_1': 2, 'padding_1': 1, 'out_channel_1': 12, 'activation_1': 'selu', 'n_layers_1': 3, 'out_features_1_0': 847, 'activation_1_0': 'hardswish', 'out_features_1_1': 344, 'activation_1_1': 'hardswish', 'out_features_1_2': 273, 'activation_1_2': 'selu', 'hidden_size': 181, 'n_layers_2': 1, 'out_features_2_0': 74, 'activation_2_0': 'selu'}. Best is trial 1 with value: 89.55769618282758.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 reward: -206.32245551767758\n",
      "CPU times: user 6h 52min 30s, sys: 1h 12min 1s, total: 8h 4min 31s\n",
      "Wall time: 19h 59min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study.optimize(objective, n_trials=10, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "offensive-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[18.434346464375864], datetime_start=datetime.datetime(2021, 3, 7, 20, 3, 46, 201516), datetime_complete=datetime.datetime(2021, 3, 7, 20, 18, 7, 553231), params={'n_layers': 1, 'kernel_size_0': 4, 'stride_0': 2, 'padding_0': 0, 'out_channel_0': 11, 'activation_0': 'hardswish', 'n_layers_1': 2, 'out_features_1_0': 842, 'activation_1_0': 'selu', 'out_features_1_1': 327, 'activation_1_1': 'hardswish', 'hidden_size': 201, 'n_layers_2': 1, 'out_features_2_0': 361, 'activation_2_0': 'hardswish'}, distributions={'n_layers': IntUniformDistribution(high=3, low=1, step=1), 'kernel_size_0': IntUniformDistribution(high=4, low=2, step=1), 'stride_0': IntUniformDistribution(high=3, low=1, step=1), 'padding_0': IntUniformDistribution(high=1, low=0, step=1), 'out_channel_0': IntUniformDistribution(high=12, low=3, step=1), 'activation_0': CategoricalDistribution(choices=('relu', 'selu', 'hardswish')), 'n_layers_1': IntUniformDistribution(high=3, low=1, step=1), 'out_features_1_0': IntUniformDistribution(high=1024, low=256, step=1), 'activation_1_0': CategoricalDistribution(choices=('relu', 'selu', 'hardswish')), 'out_features_1_1': IntUniformDistribution(high=1024, low=256, step=1), 'activation_1_1': CategoricalDistribution(choices=('relu', 'selu', 'hardswish')), 'hidden_size': IntUniformDistribution(high=327, low=163, step=1), 'n_layers_2': IntUniformDistribution(high=2, low=0, step=1), 'out_features_2_0': IntUniformDistribution(high=512, low=16, step=1), 'activation_2_0': CategoricalDistribution(choices=('relu', 'selu', 'hardswish'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast-learning prediction/policy net\n",
    "pred_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "# slow-learning target network\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(pred_net.state_dict())\n",
    "target_net.eval()  # freeze the weights of the target net\n",
    "\n",
    "\n",
    "# training loop parameters\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 1000\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "TARGET_UPDATE = 1000  # period of target network update\n",
    "optimizer = optim.RMSprop(pred_net.parameters())\n",
    "\n",
    "# TODO make weights update be after a certain step count\n",
    "\n",
    "### TRAINING LOOP ###\n",
    "num_episodes = 5\n",
    "episode_rewards = []\n",
    "steps = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen(env).to(device)\n",
    "    current_screen = get_screen(env).to(device)\n",
    "    state = current_screen - last_screen\n",
    "    episode_rewards.append(0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Select and perform an action\n",
    "        action = select_action(pred_net, state, n_actions).to(device)\n",
    "        _, reward, done, _ = env.step(action.item())  # our states are screenshot differences\n",
    "        episode_rewards[-1] += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen(env).to(device)\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model(device, pred_net, target_net, optimizer, memory)\n",
    "        steps += 1\n",
    "\n",
    "        if steps == TARGET_UPDATE:  # update the target net weights\n",
    "            steps = 0\n",
    "            target_net.load_state_dict(pred_net.state_dict())\n",
    "        \n",
    "    plot_scores(episode_rewards)\n",
    "\n",
    "print('Done')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unlimited-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 ms, sys: 3.56 ms, total: 45.2 ms\n",
      "Wall time: 43.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = [i ** 2for i in range(100000)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
